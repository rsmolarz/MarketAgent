# agents/zillow_distress_agent.py
"""
ZillowDistressAgent
-------------------
Turns Zillow Research metro datasets (ZHVI, ZORI, DOM/DOZ, inventory, sales, market temp,
affordability, etc.) into Finding objects compatible with your Finding/Regime/Council pipeline.

Primary use-case: identify *distressed* metros (liquidity freeze, affordability shock,
oversupply, rent compression) and produce actionable, IC-ready findings.

Expected CSVs (Zillow Research) — you already uploaded these:
- Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv
- Metro_zhvf_growth_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv
- Metro_new_homeowner_income_needed_downpayment_0.20_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv
- Metro_new_con_sales_count_raw_uc_sfrcondo_month.csv
- Metro_market_temp_index_uc_sfrcondo_month.csv
- Metro_mean_doz_pending_uc_sfrcondo_sm_month.csv
- Metro_sales_count_now_uc_sfrcondo_month.csv
- Metro_invt_fs_uc_sfrcondo_sm_month.csv
- Metro_zori_uc_sfrcondomfr_sm_month.csv

How to wire:
- Put this file in agents/
- Ensure BaseAgent exists in agents/base_agent.py
- Ensure Finding model exists (you shared models.py)
- Provide a helper to fetch current macro regime, or let this agent tag TA-like regimes itself.

This agent emits Findings where:
- market_type = "real_estate"
- symbol = Zillow metro RegionName
"""

from __future__ import annotations

import math
import logging
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from agents.base_agent import BaseAgent
from models import db, Finding

logger = logging.getLogger(__name__)


# -----------------------------
# Config
# -----------------------------

@dataclass
class ZillowDistressConfig:
    # where you store the Zillow Research CSVs in your repo
    data_dir: Path = Path("data/zillow")

    # history windows
    z_lookback_months: int = 60          # for z-score baseline
    yoy_months: int = 12
    mom_months: int = 1

    # scoring weights (sum ~= 1.0)
    w_price_drawdown: float = 0.25
    w_sales_collapse: float = 0.20
    w_inventory_spike: float = 0.15
    w_liquidity_freeze: float = 0.15
    w_affordability_stress: float = 0.15
    w_rent_price_divergence: float = 0.10

    # trigger thresholds
    watch_threshold: float = 0.60
    act_threshold: float = 0.75
    critical_threshold: float = 0.90

    # minimum data sufficiency
    min_points: int = 36  # months

    # emit top N metros per run (to avoid flooding)
    emit_top_n: int = 25

    # optional filters
    include_metros: Optional[List[str]] = None   # exact RegionName match list
    exclude_metros: Optional[List[str]] = None

    # if you want to downweight small/noisy markets, you can later add population weights


# -----------------------------
# Zillow Research CSV loader
# -----------------------------

def _load_zillow_wide_csv(path: Path) -> pd.DataFrame:
    """
    Zillow Research metro files are typically "wide": columns for each month.
    This returns a DataFrame where index is RegionName, columns are datetime month-ends, values float.
    """
    df = pd.read_csv(path)

    # Most Zillow Research metro files contain:
    # RegionID, SizeRank, RegionName, RegionType, StateName, ... + date columns like "2010-01-31"
    if "RegionName" not in df.columns:
        raise ValueError(f"Missing RegionName in {path.name}")

    id_cols = [c for c in df.columns if not _looks_like_date_col(c)]
    date_cols = [c for c in df.columns if _looks_like_date_col(c)]
    if not date_cols:
        raise ValueError(f"No date columns found in {path.name}")

    wide = df[["RegionName"] + date_cols].copy()
    # Parse date columns to timestamps
    dt_cols = [pd.to_datetime(c) for c in date_cols]
    wide.columns = ["RegionName"] + dt_cols
    wide = wide.set_index("RegionName").sort_index(axis=1)
    wide = wide.apply(pd.to_numeric, errors="coerce")
    return wide


def _looks_like_date_col(col: str) -> bool:
    # Zillow uses YYYY-MM-DD strings; quick check
    if len(col) != 10:
        return False
    if col[4] != "-" or col[7] != "-":
        return False
    try:
        pd.to_datetime(col)
        return True
    except Exception:
        return False


def _to_long(series_wide: pd.Series) -> pd.Series:
    """
    Convert one metro row (wide) to a time series with datetime index.
    """
    s = series_wide.dropna()
    s.index = pd.to_datetime(s.index)
    s = s.sort_index()
    return s


def _zscore_latest(ts: pd.Series, lookback: int) -> Tuple[float, float, float]:
    """
    Returns (z_latest, mean, std) computed on last `lookback` points, excluding NaNs.
    """
    ts = ts.dropna()
    if len(ts) < max(12, lookback // 2):
        return (0.0, float("nan"), float("nan"))

    window = ts.iloc[-lookback:] if len(ts) >= lookback else ts
    mu = window.mean()
    sig = window.std(ddof=0)
    if sig == 0 or np.isnan(sig):
        return (0.0, mu, sig)
    z = (ts.iloc[-1] - mu) / sig
    return (float(z), float(mu), float(sig))


def _pct_change(ts: pd.Series, periods: int) -> float:
    ts = ts.dropna()
    if len(ts) <= periods:
        return float("nan")
    prev = ts.iloc[-(periods + 1)]
    cur = ts.iloc[-1]
    if prev == 0 or np.isnan(prev) or np.isnan(cur):
        return float("nan")
    return float((cur / prev) - 1.0)


def _safe_sigmoid(x: float) -> float:
    # squash to (0,1) for compositing
    if np.isnan(x):
        return 0.5
    # cap extremes
    x = max(-8.0, min(8.0, x))
    return 1.0 / (1.0 + math.exp(-x))


# -----------------------------
# Distress regime attribution
# -----------------------------

def classify_distress_regime(features: Dict[str, float]) -> Tuple[str, float, Dict[str, float]]:
    """
    Uses interpretable rules to classify *why* a metro is distressed.
    Returns (regime_name, confidence, diagnostics)
    """
    # Inputs expected (z-scores mostly):
    # z_dom (liquidity freeze), z_inventory, z_sales (negative is bad),
    # z_afford (high bad), z_price_drawdown (negative bad), z_rent_price_div (high bad)
    z_dom = features.get("z_dom", 0.0)
    z_inv = features.get("z_inventory", 0.0)
    z_sales = features.get("z_sales", 0.0)
    z_aff = features.get("z_afford", 0.0)
    z_pr = features.get("z_price_drawdown", 0.0)
    z_rpd = features.get("z_rent_price_div", 0.0)

    # Scores for each candidate regime (higher = more likely)
    # Note: sales collapse is when sales is *low* => z_sales negative. We flip sign for that component.
    s_liq = 0.45 * _safe_sigmoid(z_dom) + 0.35 * _safe_sigmoid(-z_sales) + 0.20 * _safe_sigmoid(z_inv)
    s_aff = 0.60 * _safe_sigmoid(z_aff) + 0.25 * _safe_sigmoid(-z_sales) + 0.15 * _safe_sigmoid(z_dom)
    s_over = 0.55 * _safe_sigmoid(z_inv) + 0.25 * _safe_sigmoid(z_dom) + 0.20 * _safe_sigmoid(-z_sales)
    s_rent = 0.65 * _safe_sigmoid(z_rpd) + 0.20 * _safe_sigmoid(z_dom) + 0.15 * _safe_sigmoid(-z_sales)

    candidates = {
        "liquidity_freeze": s_liq,
        "affordability_shock": s_aff,
        "oversupply": s_over,
        "rent_compression": s_rent,
    }
    best = max(candidates, key=candidates.get)
    best_score = float(candidates[best])

    # confidence = margin between top-1 and top-2 (plus absolute strength)
    sorted_scores = sorted(candidates.values(), reverse=True)
    margin = sorted_scores[0] - (sorted_scores[1] if len(sorted_scores) > 1 else 0.0)
    conf = float(min(1.0, 0.55 * best_score + 0.45 * margin))

    return best, conf, candidates


# -----------------------------
# Recovery modeling (time-to-mean)
# -----------------------------

def estimate_time_to_mean_months(
    ts: pd.Series,
    lookback: int = 60,
    target_z: float = -0.5,
) -> Dict[str, float]:
    """
    Distress recovery modeling: estimate time for a distressed metric (e.g., price drawdown z)
    to mean-revert toward a target z-level using an OU-like half-life estimate.

    Returns:
        {
          "half_life_months": float,
          "current_z": float,
          "target_z": float,
          "eta_months": float  # expected time to reach target_z
        }

    Notes:
    - This is a pragmatic estimate: regress Δx on lagged x to infer mean reversion speed.
    - If the series is not mean-reverting (speed <= 0), we return conservative defaults.
    """
    ts = ts.dropna()
    if len(ts) < max(24, lookback // 2):
        return {"half_life_months": float("nan"), "current_z": float("nan"), "target_z": target_z, "eta_months": float("nan")}

    window = ts.iloc[-lookback:] if len(ts) >= lookback else ts
    x = window.values.astype(float)
    x1 = x[1:]
    x0 = x[:-1]
    dx = x1 - x0

    # Fit dx = a + b*x0
    # For OU, b ~ -k; k>0 => mean reversion
    A = np.vstack([np.ones_like(x0), x0]).T
    try:
        coef, *_ = np.linalg.lstsq(A, dx, rcond=None)
        a, b = float(coef[0]), float(coef[1])
    except Exception:
        a, b = 0.0, 0.0

    k = -b
    if k <= 1e-6:
        # not mean reverting; set long half-life
        half_life = 120.0
    else:
        half_life = float(math.log(2.0) / k)

    # Compute z-score on the same window
    mu = window.mean()
    sig = window.std(ddof=0) or 1.0
    current = float(window.iloc[-1])
    current_z = float((current - mu) / sig)

    # Expected time to reach target_z using exponential decay in z-space
    # z(t) ~ z0 * exp(-k t). Solve for t.
    if k <= 1e-6 or current_z == 0:
        eta = 120.0
    else:
        # if current_z already above target, eta=0
        if current_z >= target_z:
            eta = 0.0
        else:
            # move from more negative to less negative: |z| shrinking
            # ensure ratio in (0,1)
            ratio = abs(target_z) / (abs(current_z) + 1e-9)
            ratio = min(0.999, max(0.001, ratio))
            eta = float(-math.log(ratio) / k)

    return {
        "half_life_months": half_life,
        "current_z": current_z,
        "target_z": target_z,
        "eta_months": eta,
    }


# -----------------------------
# IC memo prompt (distressed real estate)
# -----------------------------

DISTRESSED_IC_MEMO_SYSTEM_PROMPT = """You are an institutional real estate investment committee (IC) analyst.
You specialize in identifying distressed housing markets and designing acquisition strategies
(REO, short sales, builder closeouts, note purchases, seller-carry, loan assumptions).

Write concise, decision-useful IC memos. Be explicit about:
- what data supports the claim
- what could falsify it
- what to do next operationally (documents, underwriting, sourcing channels)
Do not include fluff or generic macro commentary.
"""

DISTRESSED_IC_MEMO_USER_TEMPLATE = """Create an IC memo for a distressed-market opportunity.

Context
- Metro: {metro}
- Distress regime: {regime} (confidence={regime_conf:.2f})
- Distress score: {distress_score:.2f} (severity={severity})
- Key metrics (latest):
{metrics_block}

Signals & Diagnostics
- Components (0..1; higher=worse):
{components_block}

Recovery Modeling
- Estimated time-to-mean (months): {eta_months:.1f}
- Half-life (months): {half_life_months:.1f}
- Current z (distress metric): {current_z:.2f}
- Target z: {target_z:.2f}

Required Output Format (exact sections)
1) One-paragraph thesis (what is breaking and why now)
2) Distress drivers (bullets; tie to metrics)
3) Trade expression (what assets to buy; which channels; what discounts)
4) Underwriting assumptions (recovery path + base/bull/bear)
5) Kill criteria (what would prove this wrong; hard triggers)
6) Next actions (7-day sprint plan: docs, brokers, deal room checklist)
"""


# -----------------------------
# Deal kill rules tied to regime normalization
# -----------------------------

def evaluate_deal_kill_rules(
    regime: str,
    features: Dict[str, float],
    *,
    normalize_threshold: float = 0.35,
    min_confirmations: int = 2,
) -> Dict[str, object]:
    """
    Kill rules: if the distress regime is normalizing, kill/park deals
    unless you have LOI/underwriting in motion.

    normalize_threshold: below this (in component space) we consider a component normalized.
    min_confirmations: how many normalization conditions must be true to trigger a kill recommendation.

    Returns:
      {
        "kill": bool,
        "reasons": [str],
        "normalized": {metric: bool}
      }
    """
    # We look at the core stress metrics and consider them "normalized" if they are no longer stressed.
    # For z-scores, stress is typically z > +0.5 for dom/inventory/affordability and z < -0.5 for sales/price.
    z_dom = features.get("z_dom", 0.0)
    z_inv = features.get("z_inventory", 0.0)
    z_aff = features.get("z_afford", 0.0)
    z_sales = features.get("z_sales", 0.0)
    z_price = features.get("z_price_drawdown", 0.0)

    normalized = {
        "liquidity": (z_dom < 0.5) and (z_sales > -0.5),
        "oversupply": (z_inv < 0.5),
        "affordability": (z_aff < 0.5),
        "price_stabilizing": (z_price > -0.5),
    }

    # regime-specific logic
    reasons: List[str] = []
    confirmations = 0

    if regime == "liquidity_freeze":
        if normalized["liquidity"]:
            confirmations += 1
            reasons.append("Liquidity freeze normalizing: DOM stress easing and sales no longer collapsing.")
        if normalized["price_stabilizing"]:
            confirmations += 1
            reasons.append("Prices stabilizing (drawdown z improving).")

    elif regime == "oversupply":
        if normalized["oversupply"]:
            confirmations += 1
            reasons.append("Oversupply normalizing: inventory no longer elevated.")
        if normalized["liquidity"]:
            confirmations += 1
            reasons.append("Absorption improving: DOM/sales stress easing.")

    elif regime == "affordability_shock":
        if normalized["affordability"]:
            confirmations += 1
            reasons.append("Affordability stress easing (income-needed metric no longer elevated).")
        if normalized["liquidity"]:
            confirmations += 1
            reasons.append("Buyer activity returning: sales collapse easing and DOM improving.")

    elif regime == "rent_compression":
        # For rent_compression you can also check rent/price divergence z if you include it.
        z_rpd = features.get("z_rent_price_div", 0.0)
        if z_rpd < 0.5:
            confirmations += 1
            reasons.append("Rent/price divergence normalizing; cash-flow mismatch easing.")
        if normalized["liquidity"]:
            confirmations += 1
            reasons.append("Liquidity improving; distressed discounts likely compressing.")

    else:
        # unknown: be conservative
        if normalized["liquidity"] and normalized["oversupply"]:
            confirmations += 1
            reasons.append("Multiple stress indicators normalizing (liquidity + oversupply).")

    kill = confirmations >= min_confirmations
    return {"kill": kill, "reasons": reasons, "normalized": normalized}


# -----------------------------
# Agent implementation
# -----------------------------

class ZillowDistressAgent(BaseAgent):
    """
    Produces Findings for metros where macro housing indicators suggest distressed conditions.
    """

    def __init__(self, config: Optional[ZillowDistressConfig] = None):
        super().__init__()
        self.config = config or ZillowDistressConfig()

    @property
    def name(self) -> str:
        return "ZillowDistressAgent"

    def run(self) -> List[Finding]:
        cfg = self.config
        data_dir = cfg.data_dir

        # Load required series
        # NOTE: filenames can vary; adjust mapping if your repo uses different names.
        files = {
            "zhvi": data_dir / "Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv",
            "zhvi_growth": data_dir / "Metro_zhvf_growth_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv",
            "income_needed": data_dir / "Metro_new_homeowner_income_needed_downpayment_0.20_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv",
            "new_con_sales": data_dir / "Metro_new_con_sales_count_raw_uc_sfrcondo_month.csv",
            "market_temp": data_dir / "Metro_market_temp_index_uc_sfrcondo_month.csv",
            "doz_pending": data_dir / "Metro_mean_doz_pending_uc_sfrcondo_sm_month.csv",
            "sales_now": data_dir / "Metro_sales_count_now_uc_sfrcondo_month.csv",
            "inventory_fs": data_dir / "Metro_invt_fs_uc_sfrcondo_sm_month.csv",
            "zori": data_dir / "Metro_zori_uc_sfrcondomfr_sm_month.csv",
        }

        missing = [k for k, p in files.items() if not p.exists()]
        if missing:
            logger.warning(f"{self.name}: missing data files: {missing}. data_dir={data_dir.resolve()}")
            return []

        try:
            wide = {k: _load_zillow_wide_csv(p) for k, p in files.items()}
        except Exception as e:
            logger.exception(f"{self.name}: failed loading zillow datasets: {e}")
            return []

        # intersect metros across key datasets to ensure consistent scoring
        metros = set(wide["zhvi"].index)
        for k in ["sales_now", "inventory_fs", "doz_pending", "income_needed", "zori"]:
            metros &= set(wide[k].index)

        metros = sorted(list(metros))
        if cfg.include_metros:
            include = set(cfg.include_metros)
            metros = [m for m in metros if m in include]
        if cfg.exclude_metros:
            exclude = set(cfg.exclude_metros)
            metros = [m for m in metros if m not in exclude]

        scored: List[Tuple[str, float, Dict[str, float], Dict[str, float], Dict[str, object]]] = []

        for metro in metros:
            try:
                s_zhvi = _to_long(wide["zhvi"].loc[metro])
                s_sales = _to_long(wide["sales_now"].loc[metro])
                s_inv = _to_long(wide["inventory_fs"].loc[metro])
                s_dom = _to_long(wide["doz_pending"].loc[metro])  # "days on zillow pending"
                s_aff = _to_long(wide["income_needed"].loc[metro])
                s_rent = _to_long(wide["zori"].loc[metro])
            except Exception:
                continue

            # data sufficiency
            if min(len(s_zhvi), len(s_sales), len(s_inv), len(s_dom), len(s_aff), len(s_rent)) < cfg.min_points:
                continue

            # Feature engineering (z-scores on latest)
            # Price drawdown proxy: YoY change of zhvi (more negative = distressed)
            yoy_price = _pct_change(s_zhvi, cfg.yoy_months)
            yoy_rent = _pct_change(s_rent, cfg.yoy_months)
            yoy_sales = _pct_change(s_sales, cfg.yoy_months)
            yoy_inv = _pct_change(s_inv, cfg.yoy_months)
            yoy_dom = _pct_change(s_dom, cfg.yoy_months)
            yoy_aff = _pct_change(s_aff, cfg.yoy_months)

            # Convert key series to z-scores
            z_dom, *_ = _zscore_latest(s_dom, cfg.z_lookback_months)
            z_inv, *_ = _zscore_latest(s_inv, cfg.z_lookback_months)
            z_aff, *_ = _zscore_latest(s_aff, cfg.z_lookback_months)

            # sales collapse: low sales => negative yoy_sales; create series of yoy to zscore
            sales_yoy_series = s_sales.pct_change(cfg.yoy_months).dropna()
            z_sales, *_ = _zscore_latest(sales_yoy_series, cfg.z_lookback_months)

            # price drawdown: use price yoy series
            price_yoy_series = s_zhvi.pct_change(cfg.yoy_months).dropna()
            z_price_yoy, *_ = _zscore_latest(price_yoy_series, cfg.z_lookback_months)

            # rent/price divergence: if rents holding but prices falling, divergence increases
            # proxy: yoy_rent - yoy_price (higher = better rents relative to prices, can imply yield opportunity OR rent compression depending on direction)
            # For distress, we care about cash-flow mismatch / rent compression:
            # if rents weak while prices sticky OR if rents falling outright.
            rp_div_series = (s_rent.pct_change(cfg.yoy_months) - s_zhvi.pct_change(cfg.yoy_months)).dropna()
            z_rpd, *_ = _zscore_latest(rp_div_series, cfg.z_lookback_months)

            # Component scores (0..1, higher=worse)
            # Convert z-scores into distress components via sigmoid; for metrics where "lower is worse" (sales, price yoy), flip sign.
            c_price_drawdown = _safe_sigmoid(-z_price_yoy)  # negative price yoy => distressed
            c_sales_collapse = _safe_sigmoid(-z_sales)      # negative sales yoy => distressed
            c_inventory_spike = _safe_sigmoid(z_inv)
            c_liquidity_freeze = _safe_sigmoid(z_dom)
            c_affordability = _safe_sigmoid(z_aff)
            # rent/price divergence as "stress" depends; treat high negative rent relative to price as stress:
            # if rp_div is very negative, distress. We use -z_rpd.
            c_rent_price_div = _safe_sigmoid(-z_rpd)

            distress_score = (
                cfg.w_price_drawdown * c_price_drawdown
                + cfg.w_sales_collapse * c_sales_collapse
                + cfg.w_inventory_spike * c_inventory_spike
                + cfg.w_liquidity_freeze * c_liquidity_freeze
                + cfg.w_affordability_stress * c_affordability
                + cfg.w_rent_price_divergence * c_rent_price_div
            )

            features = {
                "z_dom": z_dom,
                "z_inventory": z_inv,
                "z_afford": z_aff,
                "z_sales": z_sales,
                "z_price_drawdown": -z_price_yoy,  # interpret as drawdown intensity
                "z_rent_price_div": -z_rpd,
            }

            regime, regime_conf, regime_scores = classify_distress_regime(features)

            # Recovery modeling on price_yoy_series (or use distress metric series you prefer)
            recovery = estimate_time_to_mean_months(price_yoy_series, lookback=cfg.z_lookback_months, target_z=-0.5)

            # kill rules derived from normalization
            kill_eval = evaluate_deal_kill_rules(regime, features)

            # severity mapping
            if distress_score >= cfg.critical_threshold:
                severity = "critical"
            elif distress_score >= cfg.act_threshold:
                severity = "high"
            elif distress_score >= cfg.watch_threshold:
                severity = "medium"
            else:
                severity = "low"

            # Only keep those at least WATCH
            if distress_score < cfg.watch_threshold:
                continue

            components = {
                "price_drawdown": c_price_drawdown,
                "sales_collapse": c_sales_collapse,
                "inventory_spike": c_inventory_spike,
                "liquidity_freeze": c_liquidity_freeze,
                "affordability_stress": c_affordability,
                "rent_price_divergence": c_rent_price_div,
            }

            metrics = {
                "yoy_price": yoy_price,
                "yoy_rent": yoy_rent,
                "yoy_sales": yoy_sales,
                "yoy_inventory": yoy_inv,
                "yoy_dom": yoy_dom,
                "yoy_affordability": yoy_aff,
                "latest_zhvi": float(s_zhvi.dropna().iloc[-1]),
                "latest_zori": float(s_rent.dropna().iloc[-1]),
                "latest_dom": float(s_dom.dropna().iloc[-1]),
                "latest_inventory": float(s_inv.dropna().iloc[-1]),
                "latest_sales": float(s_sales.dropna().iloc[-1]),
                "latest_income_needed": float(s_aff.dropna().iloc[-1]),
                "asof": str(s_zhvi.dropna().index[-1].date()),
            }

            scored.append((metro, float(distress_score), components, metrics, {
                "regime": regime,
                "regime_confidence": regime_conf,
                "regime_scores": regime_scores,
                "recovery": recovery,
                "kill_rules": kill_eval,
                "features": features,
            }))

        if not scored:
            return []

        # Sort and emit top N
        scored.sort(key=lambda x: x[1], reverse=True)
        scored = scored[: cfg.emit_top_n]

        findings: List[Finding] = []
        now = datetime.utcnow()

        for metro, score, components, metrics, meta in scored:
            severity = (
                "critical" if score >= cfg.critical_threshold else
                "high" if score >= cfg.act_threshold else
                "medium"
            )

            regime = meta["regime"]
            regime_conf = meta["regime_confidence"]
            recovery = meta["recovery"]
            kill_rules = meta["kill_rules"]

            title = f"Distressed Metro Signal: {metro} ({regime.replace('_',' ')})"
            description = (
                f"Distress score={score:.2f}. Regime={regime} ({regime_conf:.0%}). "
                f"Key stress: sales collapse={components['sales_collapse']:.2f}, "
                f"liquidity freeze={components['liquidity_freeze']:.2f}, "
                f"price drawdown={components['price_drawdown']:.2f}, "
                f"inventory spike={components['inventory_spike']:.2f}."
            )

            f = Finding(
                agent_name=self.name,
                timestamp=now,
                title=title,
                description=description,
                severity=severity,
                confidence=min(0.95, max(0.50, score)),  # score as confidence proxy for now
                symbol=metro,
                market_type="real_estate",
                finding_metadata={
                    "distress_score": score,
                    "distress_regime": regime,
                    "regime_confidence": regime_conf,
                    "components": components,
                    "metrics": metrics,
                    "recovery_model": recovery,
                    "kill_rules": kill_rules,
                    # for downstream (IC memo / deal room):
                    "deal_stage_suggested": "screened" if score < cfg.act_threshold else "underwritten",
                    "ic_memo_prompt": {
                        "system": DISTRESSED_IC_MEMO_SYSTEM_PROMPT,
                        "user": DISTRESSED_IC_MEMO_USER_TEMPLATE.format(
                            metro=metro,
                            regime=regime,
                            regime_conf=regime_conf,
                            distress_score=score,
                            severity=severity,
                            metrics_block=_format_kv_block(metrics),
                            components_block=_format_kv_block(components),
                            eta_months=float(recovery.get("eta_months", float("nan"))),
                            half_life_months=float(recovery.get("half_life_months", float("nan"))),
                            current_z=float(recovery.get("current_z", float("nan"))),
                            target_z=float(recovery.get("target_z", -0.5)),
                        ),
                    },
                },
            )
            findings.append(f)

        # Persist in DB if BaseAgent pattern expects that. If your framework returns findings and
        # something else commits, remove these lines.
        try:
            for f in findings:
                db.session.add(f)
            db.session.commit()
        except Exception as e:
            db.session.rollback()
            logger.exception(f"{self.name}: failed to persist findings: {e}")
            return []

        return findings


def _format_kv_block(d: Dict[str, float]) -> str:
    lines = []
    for k, v in d.items():
        if isinstance(v, float) and (np.isnan(v) or np.isinf(v)):
            vv = "NA"
        elif isinstance(v, float):
            vv = f"{v:.4f}"
        else:
            vv = str(v)
        lines.append(f"- {k}: {vv}")
    return "\n".join(lines)

# services/deal_kill_rules_wiring.py
"""
Optional wiring helpers for deal-stage progression + kill rules.

You asked for:
- screened → underwritten → LOI → closed
- automated deal kill rules tied to regime normalization

If you already have a Deal model, map these onto your model fields.
If you do NOT have a Deal model yet, keep this as a service layer and call it
from your CRM/Deal-room integration.
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, Optional, List


@dataclass
class DealState:
    deal_id: str
    metro: str
    stage: str  # screened | underwritten | loi | closed | killed
    created_at: datetime
    last_updated: datetime
    missing_docs: List[str]
    last_metrics: Dict[str, float]            # latest metrics snapshot
    last_regime: str
    last_regime_conf: float
    kill_recommended: bool = False
    kill_reasons: Optional[List[str]] = None


def advance_deal_stage(current: str, target: str) -> str:
    order = ["screened", "underwritten", "loi", "closed"]
    if current == "killed":
        return "killed"
    if target not in order:
        return current
    if current not in order:
        return target
    return target if order.index(target) >= order.index(current) else current


def apply_time_decay_kill_rule(
    deal: DealState,
    *,
    max_days_screened: int = 21,
    max_days_underwritten: int = 45,
    max_days_loi: int = 60,
) -> Optional[str]:
    """
    Kill rules based on staleness. Deals die if they don't progress.
    """
    age_days = (datetime.utcnow() - deal.created_at).days

    if deal.stage == "screened" and age_days > max_days_screened:
        return f"Time-decay kill: screened > {max_days_screened} days without progression."
    if deal.stage == "underwritten" and age_days > max_days_underwritten:
        return f"Time-decay kill: underwritten > {max_days_underwritten} days without LOI."
    if deal.stage == "loi" and age_days > max_days_loi:
        return f"Time-decay kill: LOI stage > {max_days_loi} days without close."
    return None


def apply_missing_docs_kill_rule(
    deal: DealState,
    *,
    hard_required_docs: Optional[List[str]] = None,
    grace_days: int = 10,
) -> Optional[str]:
    """
    Kill rules based on missing documentation (example list — customize for your deal room).
    """
    hard_required_docs = hard_required_docs or [
        "rent_roll",
        "trailing_12",
        "property_condition_report",
        "title_prelim",
        "insurance_loss_run",
    ]
    missing_hard = [d for d in hard_required_docs if d in (deal.missing_docs or [])]

    if not missing_hard:
        return None

    if (datetime.utcnow() - deal.created_at).days > grace_days:
        return f"Missing-docs kill: missing critical docs after {grace_days} days: {', '.join(missing_hard)}"
    return None


def apply_regime_normalization_kill_rule(
    deal: DealState,
    kill_eval: Dict[str, object],
    *,
    only_before_loi: bool = True,
) -> Optional[str]:
    """
    If the macro distress regime normalizes, the distressed discount compresses.
    Kill (or park) unless you are already at LOI/closed.
    """
    if only_before_loi and deal.stage in ("loi", "closed"):
        return None

    if kill_eval.get("kill") is True:
        reasons = kill_eval.get("reasons") or []
        return "Regime normalization kill: " + ("; ".join(reasons) if reasons else "stress indicators normalized.")
    return None


def evaluate_deal_kill_suite(
    deal: DealState,
    *,
    kill_eval: Dict[str, object],
) -> Dict[str, object]:
    """
    Run all kill rules and return a recommendation.
    """
    reasons = []

    r1 = apply_regime_normalization_kill_rule(deal, kill_eval)
    if r1:
        reasons.append(r1)

    r2 = apply_time_decay_kill_rule(deal)
    if r2:
        reasons.append(r2)

    r3 = apply_missing_docs_kill_rule(deal)
    if r3:
        reasons.append(r3)

    return {
        "kill": len(reasons) > 0,
        "reasons": reasons,
    }

# prompts/distressed_ic_memo.py
# Standalone prompt module if you prefer to keep prompts out of agents.

DISTRESSED_IC_MEMO_SYSTEM_PROMPT = """You are an institutional real estate investment committee (IC) analyst.
You specialize in identifying distressed housing markets and designing acquisition strategies
(REO, short sales, builder closeouts, note purchases, seller-carry, loan assumptions).

Write concise, decision-useful IC memos. Be explicit about:
- what data supports the claim
- what could falsify it
- what to do next operationally (documents, underwriting, sourcing channels)
Do not include fluff or generic macro commentary.
"""

DISTRESSED_IC_MEMO_USER_TEMPLATE = """Create an IC memo for a distressed-market opportunity.

Context
- Metro: {metro}
- Distress regime: {regime} (confidence={regime_conf:.2f})
- Distress score: {distress_score:.2f} (severity={severity})
- Key metrics (latest):
{metrics_block}

Signals & Diagnostics
- Components (0..1; higher=worse):
{components_block}

Recovery Modeling
- Estimated time-to-mean (months): {eta_months:.1f}
- Half-life (months): {half_life_months:.1f}
- Current z (distress metric): {current_z:.2f}
- Target z: {target_z:.2f}

Required Output Format (exact sections)
1) One-paragraph thesis (what is breaking and why now)
2) Distress drivers (bullets; tie to metrics)
3) Trade expression (what assets to buy; which channels; what discounts)
4) Underwriting assumptions (recovery path + base/bull/bear)
5) Kill criteria (what would prove this wrong; hard triggers)
6) Next actions (7-day sprint plan: docs, brokers, deal room checklist)
"""

What you need to do in your repo (minimum wiring)

    Create folder: data/zillow/ and copy your uploaded CSVs into it with these exact filenames (or edit the mapping in the agent):

    Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv

    Metro_sales_count_now_uc_sfrcondo_month.csv

    Metro_invt_fs_uc_sfrcondo_sm_month.csv

    Metro_mean_doz_pending_uc_sfrcondo_sm_month.csv

    Metro_new_homeowner_income_needed_downpayment_0.20_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv

    Metro_zori_uc_sfrcondomfr_sm_month.csv

    Add "ZillowDistressAgent" to your AVAILABLE_AGENTS list.

    Ensure your agent runner calls agent.run() and doesn’t double-commit.
    If your framework already commits, remove the db.session.add/commit block in the agent.

Deal-stage progression (screened → underwritten → LOI → closed)

This agent sets:

    finding_metadata["deal_stage_suggested"] = "screened" when WATCH-ish

    finding_metadata["deal_stage_suggested"] = "underwritten" when ACT+

Your CRM/deal-room handoff can:

    Create a Deal record at screened

    Promote to underwritten when docs checklist starts

    Promote to loi when offer sent

    Promote to closed at close

The kill-suite module is structured to run on your Deal object once it exists.