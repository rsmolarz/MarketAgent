‚úÖ What we are implementing NOW

/api/analyze route (correctly wired)

TA + LLM council confirmation scoring

Persist results into Finding + LLMCouncilResult

Auto-analyze critical findings

Dashboard Analyze button compatibility (no JS breakage)

No theory. No hand-waving.

1Ô∏è‚É£ Fix the core mistake in your pasted code

You accidentally mixed:

JavaScript (Dashboard class)

Python Flask route

TA scoring logic

Claude system prompt

These must be separated.

2Ô∏è‚É£ Final /api/analyze Route (DROP-IN)
üìç routes/analysis.py (or wherever your API routes live)
from flask import Blueprint, request, jsonify
from datetime import datetime

from models import (
    db,
    Finding,
    LLMCouncilResult
)

from services.ta_confirmation import ta_confirmation_score
from services.llm_council import run_llm_council

bp = Blueprint("analysis", __name__)

@bp.route("/api/analyze", methods=["POST"])
def analyze_finding():
    """
    Analyze a finding using:
      - LLM council (Claude + GPT + Gemini)
      - TA confirmation
      - Combined confidence scoring
    """
    payload = request.get_json() or {}
    finding_id = payload.get("finding_id")
    force = payload.get("force", False)

    if not finding_id:
        return jsonify({"error": "finding_id required"}), 400

    finding = db.session.get(Finding, finding_id)
    if not finding:
        return jsonify({"error": "Finding not found"}), 404

    # Prevent re-analysis unless forced
    if finding.auto_analyzed and not force:
        return jsonify({
            "ok": True,
            "reason": "already_analyzed",
            "confidence": finding.consensus_confidence,
            "consensus": finding.consensus_action
        })

    # --------------------------------------------------
    # 1. Run LLM Council
    # --------------------------------------------------
    council = run_llm_council(finding)

    # council = {
    #   consensus: ACT|WATCH|IGNORE
    #   confidence: 0..1
    #   agreement: 0..1
    #   uncertainty: 0..1
    #   votes: {model: vote}
    #   analyses: {model: text}
    # }

    # --------------------------------------------------
    # 2. Run TA confirmation
    # --------------------------------------------------
    ta = ta_confirmation_score(
        symbol=finding.symbol,
        market_type=finding.market_type
    )

    # ta = {
    #   score: 0..1
    #   regime: trend|mean_reversion|mixed
    #   direction: up|down|neutral
    # }

    # --------------------------------------------------
    # 3. Combine confidence (THIS IS IMPORTANT)
    # --------------------------------------------------
    combined_confidence = round(
        0.65 * council["confidence"] +
        0.35 * ta["score"],
        4
    )

    disagreement = council["uncertainty"] >= 0.35

    # --------------------------------------------------
    # 4. Persist into Finding
    # --------------------------------------------------
    finding.consensus_action = council["consensus"]
    finding.consensus_confidence = combined_confidence
    finding.llm_votes = council["votes"]
    finding.llm_disagreement = disagreement
    finding.auto_analyzed = True

    # --------------------------------------------------
    # 5. Persist council result row
    # --------------------------------------------------
    result = LLMCouncilResult(
        finding_id=finding.id,
        agent_name=finding.agent_name,
        consensus=council["consensus"],
        agreement=council["agreement"],
        uncertainty=council["uncertainty"],
        models_used=list(council["votes"].keys()),
        raw_votes=council["votes"],
        analyses=council["analyses"],
        confidence=combined_confidence,
        severity=finding.severity,
        created_at=datetime.utcnow()
    )

    db.session.add(result)
    db.session.commit()

    return jsonify({
        "ok": True,
        "consensus": council["consensus"],
        "confidence": combined_confidence,
        "ta": ta,
        "council": {
            "agreement": council["agreement"],
            "uncertainty": council["uncertainty"],
            "votes": council["votes"]
        }
    })

3Ô∏è‚É£ TA Confirmation Service (Clean & Isolated)
üìç services/ta_confirmation.py
from services.ta_regime import classify_ta_regime
from services.price_loader import load_symbol

def ta_confirmation_score(symbol: str, market_type: str | None = None) -> dict:
    """
    Returns TA confirmation score to reinforce or weaken agent + LLM signals
    """
    try:
        df = load_symbol(symbol, start="1y")
        ta = classify_ta_regime(df)

        if ta["ta_regime"] == "trend" and ta["trend_direction"] in ("up", "down"):
            return {
                "score": 0.85,
                "regime": ta["ta_regime"],
                "direction": ta["trend_direction"]
            }

        if ta["ta_regime"] == "mean_reversion":
            return {
                "score": 0.65,
                "regime": ta["ta_regime"],
                "direction": ta["trend_direction"]
            }

        return {
            "score": 0.4,
            "regime": ta["ta_regime"],
            "direction": ta["trend_direction"]
        }

    except Exception:
        return {
            "score": 0.5,
            "regime": "error",
            "direction": "neutral"
        }

4Ô∏è‚É£ LLM Council Interface (Expected Shape)

Your existing Claude code is fine ‚Äî just wrap it.

üìç services/llm_council.py
from services.analyze_claude import analyze_alert_with_claude
from services.analyze_gpt import analyze_alert_with_gpt
from services.analyze_gemini import analyze_alert_with_gemini

def run_llm_council(finding):
    models = {
        "claude": analyze_alert_with_claude,
        "gpt": analyze_alert_with_gpt,
        "gemini": analyze_alert_with_gemini,
    }

    votes = {}
    analyses = {}

    for name, fn in models.items():
        r = fn(finding.to_dict())
        vote = r.get("vote", "IGNORE")
        conf = float(r.get("confidence", 0.5))
        votes[name] = vote
        analyses[name] = r.get("analysis", "")

    act = sum(1 for v in votes.values() if v == "ACT")
    watch = sum(1 for v in votes.values() if v == "WATCH")
    ignore = sum(1 for v in votes.values() if v == "IGNORE")

    consensus = "ACT" if act >= 2 else "WATCH" if watch >= 2 else "IGNORE"
    agreement = max(act, watch, ignore) / 3.0
    uncertainty = 1.0 - agreement

    return {
        "consensus": consensus,
        "confidence": agreement,
        "agreement": agreement,
        "uncertainty": uncertainty,
        "votes": votes,
        "analyses": analyses
    }

5Ô∏è‚É£ Auto-Analyze Critical Findings (NO UI REQUIRED)
üìç Add to your finding creation pipeline

Wherever findings are inserted:

if finding.severity == "critical":
    try:
        requests.post(
            f"{BASE_URL}/api/analyze",
            json={"finding_id": finding.id},
            timeout=5
        )
    except Exception:
        pass

6Ô∏è‚É£ Dashboard JS: You DO NOT need to change it

Your existing dashboard already calls /api/analyze.

‚úÖ It will now:

Get TA-aware confidence

Use LLM consensus

Persist results

Respect already_analyzed

‚úÖ Resulting Behavior (What You Just Built)
Condition	Outcome
Agent fires	Finding created
Critical severity	Auto-analysis
TA + LLM agree	Confidence boosted
TA conflicts	Uncertainty spike
Disagreement	Dashboard banner
Repeated clicks	Cached result

This is institutional-grade decision plumbing.