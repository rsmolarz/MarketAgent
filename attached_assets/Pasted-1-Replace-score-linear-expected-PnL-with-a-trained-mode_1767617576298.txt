1) Replace score-linear expected PnL with a trained model (drop-in)
A. Add alpha/sim_model.py

Lightweight ridge regression, trains per horizon, outputs expected PnL + RMSE.

import json
from pathlib import Path
from typing import Optional, Tuple, List, Dict

import numpy as np

RECON = Path("alpha/reconciled.jsonl")
MODEL_DIR = Path("alpha/models")

# Keep small/robust; extend later
FEATURES = ["score_final", "confidence"]

def _load_jsonl(p: Path) -> List[Dict]:
    if not p.exists():
        return []
    return [json.loads(x) for x in p.read_text().splitlines() if x.strip()]

def _f(x) -> float:
    try:
        return float(x)
    except Exception:
        return 0.0

def train(horizon_hours: int = 24, ridge: float = 5.0, min_rows: int = 250, max_rows: int = 12000) -> dict:
    rows = [r for r in _load_jsonl(RECON) if int(r.get("horizon_hours", 0)) == horizon_hours]
    rows = rows[-max_rows:]
    if len(rows) < min_rows:
        return {"ok": False, "reason": "insufficient_rows", "n": len(rows), "horizon_hours": horizon_hours}

    X = []
    y = []
    for r in rows:
        X.append([_f(r.get("score_final")), _f(r.get("confidence")), 1.0])  # bias
        y.append(_f(r.get("realized_pnl_bps")))

    X = np.array(X, dtype=float)
    y = np.array(y, dtype=float)

    XtX = X.T @ X
    I = np.eye(X.shape[1], dtype=float)
    I[-1, -1] = 0.0  # don't regularize bias
    w = np.linalg.inv(XtX + ridge * I) @ (X.T @ y)

    pred = X @ w
    resid = y - pred
    rmse = float(np.sqrt(np.mean(resid ** 2)))

    MODEL_DIR.mkdir(parents=True, exist_ok=True)
    model_path = MODEL_DIR / f"ridge_h{horizon_hours}.json"
    model = {
        "ok": True,
        "model": "ridge",
        "horizon_hours": horizon_hours,
        "features": FEATURES,
        "weights": [float(x) for x in w.tolist()],
        "rmse_bps": round(rmse, 4),
        "n": len(rows),
        "ridge": ridge,
    }
    model_path.write_text(json.dumps(model, indent=2))
    return model

def load(horizon_hours: int = 24) -> Optional[dict]:
    p = MODEL_DIR / f"ridge_h{horizon_hours}.json"
    if not p.exists():
        return None
    try:
        return json.loads(p.read_text())
    except Exception:
        return None

def predict(score_final, confidence, horizon_hours: int = 24) -> Tuple[Optional[float], Optional[float], Optional[str]]:
    m = load(horizon_hours)
    if not m or not m.get("ok"):
        return None, None, None
    w = m["weights"]
    feats = [_f(score_final), _f(confidence), 1.0]
    exp = sum(feats[i] * w[i] for i in range(len(feats)))
    return float(exp), float(m.get("rmse_bps", 0.0)), f"ridge_h{horizon_hours}"

B. Patch alpha/reconcile.py to use the trained model

Replace your “expected” computation with:

from alpha.sim_model import predict as predict_expected


Then inside the horizon loop, before writing the reconciled row:

expected_pnl_bps, expected_sigma_bps, sim_version = predict_expected(
    score_final=e.get("score_final"),
    confidence=e.get("confidence"),
    horizon_hours=h
)

# Fallback if model not trained yet
if expected_pnl_bps is None:
    expected_pnl_bps = 0.0
    expected_sigma_bps = None
    sim_version = "fallback_zero"

pnl_error_bps = float(pnl_bps) - float(expected_pnl_bps)
abs_error_bps = abs(pnl_error_bps)


And add these fields to the reconciled row:

"expected_pnl_bps": round(expected_pnl_bps, 2),
"expected_sigma_bps": round(expected_sigma_bps, 2) if expected_sigma_bps is not None else None,
"pnl_error_bps": round(pnl_error_bps, 2),
"abs_error_bps": round(abs_error_bps, 2),
"sim_version": sim_version,

C. Train daily/weekly

Add a scheduled job (where you schedule supervisor) to run:

python -c "from alpha.sim_model import train; import json; print(json.dumps(train(24), indent=2))"


If you want multi-horizon, train 1,4,24.

2) Regime-conditioned CVaR thresholds
A. Extend strategy attribution to compute CVaR by regime

Create meta_supervisor/strategy_cvar_regime.py:

import json
from pathlib import Path
from collections import defaultdict

RECON = Path("alpha/reconciled.jsonl")
OUT = Path("meta_supervisor/state/strategy_cvar_regime.json")

def _load_jsonl(p: Path):
    if not p.exists():
        return []
    return [json.loads(x) for x in p.read_text().splitlines() if x.strip()]

def cvar(values, alpha=0.95):
    if not values:
        return 0.0
    vs = sorted(values)
    cutoff = int(len(vs) * alpha)
    tail = vs[cutoff:]
    return sum(tail) / max(len(tail), 1)

def run(horizon_hours: int = 24):
    rows = [r for r in _load_jsonl(RECON) if int(r.get("horizon_hours", 0)) == horizon_hours]
    rows = rows[-8000:]

    by_key = defaultdict(list)  # (strategy, regime) -> [pnl_error_bps]
    for r in rows:
        strat = r.get("strategy_class") or "unknown"
        regime = r.get("regime") or "unknown"
        pe = r.get("pnl_error_bps")
        if pe is None:
            continue
        by_key[(strat, regime)].append(float(pe))

    out = {}
    for (s, g), vals in by_key.items():
        out.setdefault(s, {})[g] = {
            "n": len(vals),
            "cvar95_error_bps": round(cvar(vals, 0.95), 2),
            "mean_error_bps": round(sum(vals)/max(len(vals),1), 2),
        }

    OUT.parent.mkdir(parents=True, exist_ok=True)
    OUT.write_text(json.dumps(out, indent=2))
    return out

B. Patch reconcile to include strategy_class

Right now you only have agent. Add this mapping once:

In meta_supervisor/agent_registry.py you already have AGENT_STRATEGY_CLASS.
Import it in reconcile and set:

from meta_supervisor.agent_registry import AGENT_STRATEGY_CLASS
strategy_class = AGENT_STRATEGY_CLASS.get(e.get("agent"), "unknown")


Then add "strategy_class": strategy_class to the reconciled row.

C. Add thresholds config meta_supervisor/strategy_kill_list.yaml

Example:

macro:
  by_regime:
    TREND:
      max_cvar_error_bps: 120
    RANGE:
      max_cvar_error_bps: 90
arb:
  by_regime:
    TREND:
      max_cvar_error_bps: 140
    RANGE:
      max_cvar_error_bps: 110

D. Enforce in evaluate_strategy_thresholds()

In your current strategy_attribution module, add:

load by_regime thresholds

if current regime CVaR breaches, mark strategy DISABLED.

3) Divergence alerts when sim vs live breaks (email)
A. Create meta_supervisor/divergence_alerts.py

Triggers on:

rolling mean abs_error_bps > threshold

OR rmse spike

OR “live hit rate” collapses vs expected

import json
import os
from pathlib import Path
from datetime import datetime, timezone
from collections import defaultdict

RECON = Path("alpha/reconciled.jsonl")
ALERTS = Path("meta_supervisor/state/divergence_alerts.json")

def _now():
    return datetime.now(timezone.utc).isoformat().replace("+00:00","Z")

def _load_jsonl(p: Path):
    if not p.exists():
        return []
    return [json.loads(x) for x in p.read_text().splitlines() if x.strip()]

def detect(window: int = 80, horizon_hours: int = 24, abs_err_thr: float = 90.0, mean_err_thr: float = 40.0):
    rows = [r for r in _load_jsonl(RECON) if int(r.get("horizon_hours", 0)) == horizon_hours]
    rows = rows[-8000:]

    by_agent = defaultdict(list)
    for r in rows:
        by_agent[r.get("agent","unknown")].append(r)

    alerts = []
    for agent, rs in by_agent.items():
        tail = rs[-window:]
        abs_err = [float(x.get("abs_error_bps",0) or 0) for x in tail if x.get("abs_error_bps") is not None]
        err = [float(x.get("pnl_error_bps",0) or 0) for x in tail if x.get("pnl_error_bps") is not None]
        if len(abs_err) < max(10, window//4):
            continue
        m_abs = sum(abs_err)/len(abs_err)
        m_err = sum(err)/len(err)
        if m_abs >= abs_err_thr or abs(m_err) >= mean_err_thr:
            alerts.append({
                "ts": _now(),
                "agent": agent,
                "window": len(abs_err),
                "mean_abs_error_bps": round(m_abs, 2),
                "mean_error_bps": round(m_err, 2),
                "severity": "high" if m_abs >= abs_err_thr*1.3 else "medium",
                "action": "REDUCE_ALLOC_AND_BLOCK_PROMOTION"
            })

    ALERTS.parent.mkdir(parents=True, exist_ok=True)
    ALERTS.write_text(json.dumps({"generated_at": _now(), "alerts": alerts}, indent=2))
    return alerts

B. Email the alerts (reuse your SendGrid service)

In your supervisor run, if alerts non-empty:

include in PR comment

send an email “DIVERGENCE ALERT” to META_EMAIL_TO

4) Auto-open GitHub PRs when strategy classes are disabled

You already have a maybe_create_pr(report) hook. Extend it:

A. When breaches exist, write a PR that:

Updates meta_supervisor/strategy_kill_list.yaml with status: DISABLED

Adds the breach reasons & timestamp

B. Create meta_supervisor/github_pr.py behavior:

If any strategy breaches → open PR

Title: [AUTO] Disable strategy: <strategy>

Body: include CVaR + hit rate + pnl + regime

If you don’t already have GitHub API wired in Replit:

use a GitHub token secret GITHUB_TOKEN

use REST API to create branch + commit + PR.

Minimal “do it now” plan (so it actually lands)

Add alpha/sim_model.py

Patch alpha/reconcile.py to write: expected_pnl_bps, abs_error_bps, pnl_error_bps, sim_version, strategy_class

Add a scheduled train() (daily)

Add meta_supervisor/divergence_alerts.py and call it in supervisor

Add strategy_cvar_regime.py + thresholds enforcement

Extend GitHub PR creation to fire when disabling strategies

One blocker you must resolve

Your regime-conditioned CVaR requires that reconciled rows include:

strategy_class

regime

You have regime already in alpha event, but I don’t see strategy_class being persisted. Add it in reconciliation as described above.

If you paste your current meta_supervisor/github_pr.py (or whatever creates PRs today), I’ll give you the exact diff to make “disable strategy → open PR” work without guesswork.